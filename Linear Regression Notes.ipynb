{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "446d2849",
   "metadata": {},
   "source": [
    "## Linear Regression Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c321983",
   "metadata": {},
   "source": [
    "### Simple Linear Rigression is completely dependent on the independent and the dependent variables\n",
    "### It is determined by a straight line drawn through the number of random independent variables (x coordinate) \n",
    "### dependent variables (Y coordinates) and then determine the values of the m ( slope) and c (constant ). Based on by the given datapoint we try to plot the best line to fit the data. The line can be drawn based on linear equation y = mx + c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc44e4",
   "metadata": {},
   "source": [
    "### Motive of the linear rigression is to find the values of m and c. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a16e94",
   "metadata": {},
   "source": [
    "### Gradients are determined by the partial differentiation of the m and c values. The alpha is used to determine the hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd6c4d",
   "metadata": {},
   "source": [
    "## Learning the Data\n",
    "#### Learning the data is by means of training and testing them. Later on the x variables which are the independent once are further undergone through the Linear Rigression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231ba7a",
   "metadata": {},
   "source": [
    "### How the training and testing of the model is done ?\n",
    "\n",
    "#### The model can be fit in the training data and predict it in the testing data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d9ef1",
   "metadata": {},
   "source": [
    "### How the accuracy of the model is predicted ?\n",
    "\n",
    "#### The accuracy of the model can be predicted through the R2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e1bb9",
   "metadata": {},
   "source": [
    "## What are the types of Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843f258",
   "metadata": {},
   "source": [
    "### 1. Linear regression\n",
    "\n",
    "One of the most basic types of regression in machine learning, linear regression comprises a predictor variable and a dependent variable related to each other in a linear fashion. Linear regression involves the use of a best fit line, as described above.\n",
    "\n",
    "You should use linear regression when your variables are related linearly. For example, if you are forecasting the effect of increased advertising spend on sales. However, this analysis is susceptible to outliers, so it should not be used to analyze big data sets.\n",
    "\n",
    "2. Logistic regression\n",
    "\n",
    "Does your dependent variable have a discrete value? In other words, can it only have one of two values (either 0 or 1, true or false, black or white, spam or not spam, and so on)? In that case, you might want to use logistic regression to analyze your data.\n",
    "\n",
    "Logistic regression uses a sigmoid curve to show the relationship between the target and independent variables. However, caution should be exercised: logistic regression works best with large data sets that have an almost equal occurrence of values in target variables. The dataset should not contain a high correlation between independent variables (a phenomenon known as multicollinearity), as this will create a problem when ranking the variables.\n",
    "\n",
    "3. Ridge regression\n",
    "\n",
    "If, however, you do have a high correlation between independent variables, ridge regression is a more suitable tool. It is known as a regularization technique, and is used to reduce the complexity of the model. It introduces a small amount of bias (known as the ‘ridge regression penalty’) which, using a bias matrix, makes the model less susceptible to overfitting.\n",
    "\n",
    "4. Lasso regression\n",
    "\n",
    "Like ridge regression, lasso regression is another regularization technique that reduces the model’s complexity. It does so by prohibiting the absolute size of the regression coefficient. This causes the coefficient value to become closer to zero, which does not happen with ridge regression.\n",
    "\n",
    "The advantage? It can use feature selection, letting you select a set of features from the dataset to build the model. By only using the required features – and setting the rest as zero – lasso regression avoids overfitting.\n",
    "\n",
    "5. Polynomial regression\n",
    "\n",
    "Polynomial regression models a non-linear dataset using a linear model. It is the equivalent of making a square peg fit into a round hole. It works in a similar way to multiple linear regression (which is just linear regression but with multiple independent variables), but uses a non-linear curve. It is used when data points are present in a non-linear fashion.\n",
    "\n",
    "The model transforms these data points into polynomial features of a given degree, and models them using a linear model. This involves best fitting them using a polynomial line, which is curved, rather than the straight line seen in linear regression. However, this model can be prone to overfitting, so you are advised to analyze the curve towards the end to avoid odd-looking results.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747f2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
